# 평균 제곱 오차 (Mean Squared Error, MSE)

> 최소 제곱법을 사용하지 않고 아무 값이나 a와 b에 대입합니다.
> $$$\hat{y} = ax + b$$$
> $$$error = y - \hat{y}$$$
> $$$MSE = \frac{1}{n} \sum_{i=0}^{n} error^{2} $$$
> &nbsp;
> MSE를 최소화하는 방법을 통하여서 가장 오차가 적은 예측선을 찾는다.
> Error를 바로 쓰지 않고 MSE로 사용하는 이유는,
> 음수, 양수가 섞여있기 때문에 정확한 값을 측정하기 위해서 제곱 후 $$$\sum$$$ 해준다.


```mermaid
graph LR
A(임의로 a, b를 정한다. )-->B[y = ax + b<br/> 직선을 그린다.]
B-->C[오차를 확인한다.]
C-->D(a와 b를 update 후 <br/> 앞의 process 반복)
```


# 경사하강법 (gradient descent)
>오차를 비교하여 가장 작은 방향으로 이동시키는 방법
>(미분 : 한 점에서 순간 기울기)
>&nbsp;
>기울기 a를 너무 크게 잡거나, 작게 잡으면 오차가 커진다.

1. a~1~에서 미분을 구한다.
2. 구해진 기울기의 반대방향 $$$ (- \rightarrow +) $$$ or $$$ (+ \rightarrow  -) $$$으로 얼마간 이동시킨 a~2~에서 미분을 구한다.
3. 위에서 구한 미분 값이 0이 아니면 위 과정을 반복한다.

# 학습률 (Learning rate)

1. 부호 바꾸는 작업할 때 거리를 너무 크게 잡으면, a값 한 점에 모이지 않고 위로 치솟는다.
2. 이동을 얼마나 할지 신중히 정해야함. (이게 바로 학습율임)



## Ex) 02_.py

|공부시간(x)|2|4|6|8
|:--|:--:|:--:|:--:|:--:
|성적(실제 값, y)|81|93|91|97
|예측 값|82|88|94|100
|Error|1|-5|3|3|

>실제 값과 오차가 제일 작은 예측값을 찾기위해
>우리가 궁금한 것은 MSE가 최소일 때의 a와 b이다.

&nbsp;

0. 초기값 설정
> 공부시간 : x, 성적 : y
> numpy 배열로 변환 (인덱스를 주어 하나씩 불러와 계산하기 위함(내부함수 활용))
> a = 0, b = 0
> lr(학습률) = 0.03
> epochs = 2001 (최대 반복회수 설정)

1. 임의의 `기울기 : a`,  `y절편 : b`인 직선을 그린다.
```
a : fake_a_b[0],
b : fake_a_b[1]
```
2. $$$ \hat{y} $$$를 구한다. -> $$$\hat{y}$$$를 predict_result에 하나하나 넣는다.
```
def predict(x):
	return fake_a_b[0] * x + fake_a_b[1]
```
3. 실제 값과 예측값의 차이 즉, MSE를 구한다. // 코드 수정 요구
```
def mse(y, y_hat):
	return 1/n(sum(y - y_hat)**2)
```

4. a와 b의 값 업데이트하기 (업데이트 후 다시 3번으로 epchs만큼 반복)
&nbsp;
$$$\frac{1}{n}\sum{(y_{i} - (ax_{i} + b))^{2}}$$$를 a와 b를 중심으로 편미분하여 update할 값을 구한다.
	- a = a - lr(학습률) * a_diff
	- b = b - lr * b_diff

i) a로 편미분 : a_diff
$$
\frac{d}{da}(\frac{1}{n}\sum{(y_{i} - (ax_{i} + b))^{2}})
= \frac{2}{n}\sum{(y_{i} - (ax_i + b))(-x_i)} = \frac{2}{n}\sum{((ax_i + b)- y_{i})(x_i)}
$$

ii) b로 편미분 : b_diif
$$
\frac{d}{da}(\frac{1}{n}\sum{(y_{i} - (ax_{i} + b))^{2}})
= \frac{2}{n}\sum{(y_{i} - (ax_i + b))(-1)} = \frac{2}{n}\sum{((ax_i + b)- y_{i})}
$$

>최종결과 : a = 2.3에 수렴, b가 79에 수렴한다.

