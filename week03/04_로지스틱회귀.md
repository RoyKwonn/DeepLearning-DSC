# 로지스틱 회귀(Logistic regression) : 참, 거짓을 판단하는 장치

>딥러닝을 수행한다는 것은 
>外로 들어나지 않는
>'미니 판단 장치'(model)가 작동하여
>최적의 예측값 出

- 공부시간 -> 성적   (선형회기 : 직선)
- 공부시간 -> 合/不  (로지스틱회기 : binary)


# 시그모이드 함수 (Sigmoid function)

$$$y = \frac{1}{1 + e^{-(ax + b)}} = (1 + e^{-(ax + b)})^{-1}$$$

&nbsp;

시그모이드에서 오차가 커지는 케이스
1. 실제 : 0 -> 예측 : 1
$$$- \log (1 - h)$$$
2. 실제 : 1 -> 예측 : 0
$$$- \log (h) $$$

&nbsp;
case 1, 2를 합친 공식은 아래와 같다.
$$$-\{y \log h + (1 - y) log (1 - h)\}$$$
> y가 0이면, + 뒤만 작용하고
> y가 1이면, + 앞만 작용한다.

이 식에서 a와 b를 편미분 하는 것은 꽤 복잡한 수학의 영역이다.


## Ex)

|공부시간|2|4|6|8|10|12|14
|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|합격 여부|不|不|不|合|合|合|合|

>우리가 구해야 하는 값 : $$$ax + b$$$
>경사 하강법으로 구한다.
>(오차를 구한 다음, 오차가 작은 쪽으로 이동시키는 방법)
